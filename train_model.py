# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sqDIBp1iLEcQ0zl26xrn8zH56wpdI1IQ
"""

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

data = pd.read_csv('ner_dataset.csv',encoding = 'latin1')
data.head()

data = data.fillna(method = 'ffill')

data

import tqdm

def sentence_integrate(data):
  agg_func = lambda s: [(w, p, t) for w, p, t in zip(s["Word"].values.tolist(),
                                                     s["POS"].values.tolist(),
                                                     s["Tag"].values.tolist())]
  return data.groupby('Sentence #').apply(agg_func).tolist()

sentences = sentence_integrate(data) #creates a list of tuples for each sentence. The tuples contain each token in the sentence along with the pos and the tag of the token

sentences[0]

len(sentences)

words = data['Word'].unique().tolist()
words.append('ENDPAD')
tags = data['Tag'].unique().tolist()
tags_count = len(tags)
words_count = len(words)

print(tags) #these are the tags we want tthe model to classify the tokens into

#simple mapping of word and tag to their indices
word2idx = {w: i + 1 for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}

tag2idx

#padding/truncating all the sentences to be of the same length because model expects all inputs to be of same length.
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_len = 50

# extract sentence words
X = [[word2idx[w[0]] for w in s] for s in sentences]
X = pad_sequences(maxlen=max_len, sequences=X, padding="post", value=words_count-1)

# extract sentence tags
y = [[tag2idx[w[2]] for w in s] for s in sentences]
y = pad_sequences(maxlen=max_len, sequences=y, padding="post", value=tag2idx["O"])

#X and y are 2d matrices which hold the index representation of the words in each sentecne and the Tag associated with each token in the sentence

(X[0])

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, SpatialDropout1D, Bidirectional

# Define model constants
MAX_LEN = 50
WORDS_COUNT = len(words)  # Assuming 'words' list is defined from your previous code
TAGS_COUNT = len(tags)    # Assuming 'tags' list is defined from your previous code

# Build the model
model = keras.Sequential([
    # Add an explicit Input layer to define the shape
    keras.Input(shape=(MAX_LEN,)),

    # The Embedding layer now infers its input length from the Input layer
    Embedding(input_dim=WORDS_COUNT, output_dim=128),

    SpatialDropout1D(0.2),

    # Bidirectional LSTM to learn from both past and future context.
    # Note: The warning about not using cuDNN kernels is expected when using recurrent_dropout.
    # The model will still train correctly on a GPU, just using a different kernel.
    Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.2)),

    # The TimeDistributed layer applies a Dense layer to every token in the sequence.
    TimeDistributed(Dense(TAGS_COUNT, activation="softmax"))
])

# Compile the model
model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# Now, the model summary will be fully populated!
print("Model Summary:")
model.summary()

tf.keras.utils.plot_model(
    model, to_file='model.png', show_shapes=True, show_dtype=False,
    show_layer_names=True, rankdir='LR', expand_nested=True, dpi=300,
)

#compile model
model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# pip install livelossplot

tf.device('/device:GPU:0')

# --- Callbacks and Training ---

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, SpatialDropout1D, Bidirectional
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Set up logging for TensorBoard
logdir="log/"
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

# Corrected ModelCheckpoint: filepath now ends in ".weights.h5"
chkpt = ModelCheckpoint("model_weights.weights.h5",
                        monitor='val_loss',
                        verbose=1,
                        save_best_only=True,
                        save_weights_only=True,
                        mode='min')

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_accuracy',
                             min_delta=0,
                             patience=2, # Increased patience slightly
                             verbose=1,
                             mode='max',
                             baseline=None,
                             restore_best_weights=True)

# Define the list of callbacks

callbacks = [chkpt, early_stopping, tensorboard_callback]

# Train the model
print("\nStarting model training...")
history = model.fit(
    x=x_train,
    y=y_train,
    validation_data=(x_test, y_test),
    batch_size=64,
    epochs=15, # Increased epochs, early stopping will handle the rest
    callbacks=callbacks,
    verbose=1
)
print("Model training finished.")

# --- Prediction Function ---

# Create a reverse mapping from index to tag to decode predictions
idx2tag = {i: w for w, i in tag2idx.items()}


def predict_entities(sentence): 
    """
    Predicts named entities in a sentence and prints them.
    """
    print(f"\n--- Predictions for: '{sentence}' ---")

    # 1. Tokenize the sentence
    words_in_sentence = sentence.split()

    # 2. Convert words to their indices (use 0 for unknown words)
    word_indices = [word2idx.get(w, 0) for w in words_in_sentence]

    # 3. Pad the sequence to MAX_LEN
    padded_sequence = pad_sequences([word_indices], maxlen=MAX_LEN, padding="post", value=WORDS_COUNT-1)

    # 4. Get model predictions
    predictions = model.predict(padded_sequence)
    # 5. Get the tag index with the highest probability for each token
    predicted_indices = np.argmax(predictions, axis=-1)

    # 6. Print the results
    print(f"{'Word':<18} {'Predicted Tag'}")
    print("="*32)
    for i, w in enumerate(words_in_sentence):
        # Ensure we don't go out of bounds
        if i < len(predicted_indices[0]):
            predicted_tag = idx2tag[predicted_indices[0][i]]
            # Only print meaningful tags
            if predicted_tag != 'O':
                 print(f"{w:<18} {predicted_tag}")

# Prediction test
print("\nLoading best weights for prediction...")
model.load_weights("model_weights.weights.h5")

predict_entities("The Eiffel Tower is in Paris France")
predict_entities("President Biden will visit London next week")

import json

print("\nSaving word and tag mappings to ner_mappings.json...")
# Also create idx2tag for the API to use
idx2tag_for_api = {i: w for w, i in tag2idx.items()}

mappings = {
    "word2idx": word2idx,
    "tag2idx": tag2idx,
    "idx2tag": idx2tag_for_api,
    "max_len": MAX_LEN,
    "words_count": words_count, # Note: using lowercase variable name
    "tags_count": tags_count   # Note: using lowercase variable name
}

with open('ner_mappings.json', 'w') as f:
    # Use json.dump to write the dictionary to a file
    json.dump(mappings, f, indent=4)

print("Mappings saved successfully.")
# %%
